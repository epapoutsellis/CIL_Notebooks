{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from cil.optimisation.algorithms import PDHG\n",
    "from cil.optimisation.functions import L2NormSquared, MixedL21Norm, BlockFunction, IndicatorBox, TotalVariation\n",
    "from cil.optimisation.operators import BlockOperator, GradientOperator\n",
    "from cil.framework import ImageGeometry, AcquisitionGeometry, AcquisitionData, BlockDataContainer\n",
    "from cil.plugins.astra.operators import ProjectionOperator\n",
    "from cil.plugins.astra.processors import FBP\n",
    "from cil.plugins.ccpi_regularisation.functions import FGP_TV  \n",
    "from cil.utilities.display import show2D\n",
    "\n",
    "import numpy as np\n",
    "import os, sys\n",
    "\n",
    "import tomophantom\n",
    "from tomophantom import TomoP2D\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tomophantom image \n",
    "model = 8 # select a model number from the library\n",
    "N = 256 # set dimension of the phantom\n",
    "path = os.path.dirname(tomophantom.__file__)\n",
    "path_library2D = os.path.join(path, \"Phantom2DLibrary.dat\")\n",
    "\n",
    "phantom2D_np = TomoP2D.Model(model, N, path_library2D)    \n",
    "ig = ImageGeometry(voxel_num_x=N, voxel_num_y=N, voxel_size_x = 0.5, voxel_size_y = 0.5)\n",
    "\n",
    "phantom2D = ig.allocate()\n",
    "phantom2D.fill(phantom2D_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Acquisition Geometry\n",
    "detectors =  int(np.sqrt(2)*N)\n",
    "angles = np.linspace(0, np.pi, 90, dtype=np.float32)\n",
    "\n",
    "ag = AcquisitionGeometry.create_Parallel2D()\\\n",
    "                        .set_angles(angles,angle_unit=\"radian\")\\\n",
    "                        .set_panel(detectors, pixel_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Projection Operator\n",
    "A = ProjectionOperator(ig, ag, device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create projection data and corrupt with noise\n",
    "np.random.seed(10)\n",
    "\n",
    "data = A.direct(phantom2D)\n",
    "noisy_data = ag.allocate()\n",
    "noisy_data.fill(data.as_array() + np.random.normal(0, 0.5, ag.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularisation parameter\n",
    "alpha = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and run PDHG (explicit) algorithm\n",
    "f = BlockFunction(0.5 * L2NormSquared(b=noisy_data), alpha * MixedL21Norm())\n",
    "g = IndicatorBox(lower=0.0)\n",
    "\n",
    "Grad = GradientOperator(ig)\n",
    "K = BlockOperator(A, Grad)\n",
    "\n",
    "normK = K.norm()\n",
    "sigma = 1.\n",
    "tau = 1./(sigma*normK**2)\n",
    "\n",
    "pdhg_explicit = PDHG(f=f, g=g, operator=K, sigma=sigma, tau=tau,\n",
    "           max_iteration=2000, update_objective_interval=500)\n",
    "pdhg_explicit.run(verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and run PDHG (implicit) algorithm, using TotalVariation function (cpu)\n",
    "normK = A.norm()\n",
    "\n",
    "f1 = 0.5 * L2NormSquared(b=noisy_data)\n",
    "g1 = alpha*TotalVariation(max_iteration=100,lower=0.) \n",
    "K1 = A\n",
    "normK = A.norm()\n",
    "\n",
    "sigma = 1./normK\n",
    "tau = 1./normK\n",
    "\n",
    "pdhg_implicit_cpu = PDHG(f=f1, g=g1, operator=K1, sigma=sigma, tau=tau,\n",
    "           max_iteration=1000, update_objective_interval=200)\n",
    "pdhg_implicit_cpu.run(verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and run PDHG (implicit) algorithm, using FGP_TV function from the CCPiRegularisation toolkit (gpu)\n",
    "g2 = (alpha/ig.voxel_size_x)*FGP_TV(device=\"gpu\", max_iteration=100)\n",
    "\n",
    "pdhg_implicit_gpu = PDHG(f=f1, g=g2, operator=K1, sigma=sigma, tau=tau,\n",
    "           max_iteration=500, update_objective_interval=100)\n",
    "pdhg_implicit_gpu.run(verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D([phantom2D, pdhg_explicit.solution, pdhg_implicit_cpu.solution,  \n",
    "        pdhg_implicit_gpu.solution, (pdhg_explicit.solution-pdhg_implicit_cpu.solution).abs(),\n",
    "       (pdhg_implicit_cpu.solution-pdhg_implicit_gpu.solution).abs()],num_cols=3, size=(20,10),\n",
    "          title=[\"Phantom\", \"PDHG (explicit)\", \"PDHG (implicit-cpu) \", \n",
    "                 \"PDHG (implicit-gpu) \", \"AbsDif Explicit vs Implicit(cpu)\", \"AbsDif Implicit(cpu) vs Implicit(gpu)\"],\n",
    "         origin = \"upper\", cmap=\"inferno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cil.optimisation.functions import Function\n",
    "# import numpy\n",
    "# from numbers import Number\n",
    "\n",
    "# class new_TotalVariation(Function):\n",
    "    \n",
    "#     r'''Fast Gradient Projection algorithm for Total Variation(TV) Denoising (ROF problem)\n",
    "    \n",
    "#     .. math::  \\min_{x} \\alpha TV(x) + \\frac{1}{2}||x-b||^{2}_{2}\n",
    "                \n",
    "#     Parameters:\n",
    "      \n",
    "#       :param max_iteration: max iterations of FGP algorithm\n",
    "#       :type max_iteration: int, default 100\n",
    "#       :param tolerance: Stopping criterion\n",
    "#       :type tolerance: float, default `None` \n",
    "#       :param correlation: Correlation between `Space` and/or `SpaceChannels` for the GradientOperator\n",
    "#       :type correlation: str, default 'Space'\n",
    "#       :param backend: Backend to compute finite differences for the GradientOperator\n",
    "#       :type backend: str, default 'c'\n",
    "#       :param lower:lower bound for the orthogonal projection onto the convex set C\n",
    "#       :type lower: Number, default `-numpy.inf`\n",
    "#       :param upper: upper bound for the orthogonal projection onto the convex set C\n",
    "#       :type upper: Number, default `+numpy.inf`\n",
    "#       :param info: force a print to screen stating the stop\n",
    "#       :type info: bool, default `False`\n",
    "#     Reference:\n",
    "      \n",
    "#         A. Beck and M. Teboulle, \"Fast Gradient-Based Algorithms for Constrained Total Variation \n",
    "#         Image Denoising and Deblurring Problems,\" in IEEE Transactions on Image Processing,\n",
    "#         vol. 18, no. 11, pp. 2419-2434, Nov. 2009, \n",
    "#         doi: 10.1109/TIP.2009.2028250.\n",
    "        \n",
    "#     '''    \n",
    "    \n",
    "    \n",
    "#     def __init__(self,\n",
    "#                  max_iteration=100, \n",
    "#                  tolerance = None, \n",
    "#                  correlation = \"Space\",\n",
    "#                  backend = \"c\",\n",
    "#                  lower = -numpy.inf, \n",
    "#                  upper = numpy.inf,\n",
    "#                  info = False):\n",
    "        \n",
    "\n",
    "#         super(new_TotalVariation, self).__init__(L = None)\n",
    "#         # Regularising parameter = alpha\n",
    "#         self.regularisation_parameter = 1.\n",
    "        \n",
    "#         # Iterations for FGP_TV\n",
    "#         self.iterations = max_iteration\n",
    "        \n",
    "#         # Tolerance for FGP_TV\n",
    "#         self.tolerance = tolerance\n",
    "        \n",
    "#         # Define (ISOTROPIC) Total variation penalty ( Note it is without the regularisation paremeter)\n",
    "#         # TODO add anisotropic???\n",
    "#         self.TV = MixedL21Norm() \n",
    "                \n",
    "#         # correlation space or spacechannels\n",
    "#         self.correlation = correlation\n",
    "#         self.backend = backend        \n",
    "        \n",
    "#         # Define orthogonal projection onto the convex set C\n",
    "#         self.lower = lower\n",
    "#         self.upper = upper\n",
    "#         self.tmp_proj_C = IndicatorBox(lower, upper).proximal\n",
    "                        \n",
    "# #         Setup GradientOperator as None. This is to avoid domain argument in the __init__     \n",
    "\n",
    "#         self._gradient = None\n",
    "#         self._domain = None\n",
    "\n",
    "#         self.pptmp = None\n",
    "#         self.pptmp1 = None\n",
    "                     \n",
    "#         # Print stopping information (iterations and tolerance error) of FGP_TV  \n",
    "#         self.info = info\n",
    "#     @property\n",
    "#     def regularisation_parameter(self):\n",
    "#         return self._regularisation_parameter\n",
    "\n",
    "#     @regularisation_parameter.setter\n",
    "#     def regularisation_parameter(self, value):\n",
    "#         if not isinstance(value, Number):\n",
    "#             raise TypeError(\"regularisation_parameter: expectec a number, got {}\".format(type(value)))\n",
    "#         self._regularisation_parameter = value\n",
    "\n",
    "#     def __call__(self, x):\n",
    "        \n",
    "#         r''' Returns the value of the \\alpha * TV(x)'''\n",
    "#         self._domain = x.geometry\n",
    "#         # evaluate objective function of TV gradient\n",
    "#         return self.regularisation_parameter * self.TV(self.gradient.direct(x))\n",
    "    \n",
    "    \n",
    "#     def projection_C(self, x, out=None):   \n",
    "                     \n",
    "#         r''' Returns orthogonal projection onto the convex set C'''\n",
    "\n",
    "#         self._domain = x.geometry\n",
    "#         return self.tmp_proj_C(x, tau = None, out = out)\n",
    "                        \n",
    "#     def projection_P(self, x, out=None):\n",
    "                       \n",
    "#         r''' Returns the projection P onto \\|\\cdot\\|_{\\infty} '''  \n",
    "#         self._domain = x.geometry\n",
    "        \n",
    "#         # preallocated in proximal\n",
    "#         tmp = self.pptmp\n",
    "#         tmp1 = self.pptmp1\n",
    "#         tmp1 *= 0\n",
    "        \n",
    "\n",
    "#         for i,el in enumerate(x.containers):\n",
    "#             el.multiply(el, out=tmp)\n",
    "#             tmp1.add(tmp, out=tmp1)\n",
    "#         tmp1.sqrt(out=tmp1)\n",
    "#         tmp1.maximum(1.0, out=tmp1)\n",
    "#         if out is None:\n",
    "#             return x.divide(tmp1)\n",
    "#         else:\n",
    "#             x.divide(tmp1, out=out)\n",
    "    \n",
    "    \n",
    "#     def proximal(self, x, tau, out = None):\n",
    "        \n",
    "#         ''' Returns the solution of the FGP_TV algorithm '''         \n",
    "#         self._domain = x.geometry\n",
    "        \n",
    "#         # initialise\n",
    "#         t = 1        \n",
    "#         tmp_p = self.gradient.range_geometry().allocate(0)  \n",
    "#         self.tmp_q = tmp_p.copy()\n",
    "#         tmp_x = self.gradient.domain_geometry().allocate(None)     \n",
    "#         p1 = self.gradient.range_geometry().allocate(None)\n",
    "        \n",
    "\n",
    "#         should_break = False\n",
    "#         for k in range(self.iterations):\n",
    "                                                                                   \n",
    "#             t0 = t\n",
    "#             self.gradient.adjoint(self.tmp_q, out = tmp_x)\n",
    "            \n",
    "#             # axpby now works for matrices\n",
    "#             tmp_x.axpby(-self.regularisation_parameter*tau, 1.0, x, out=tmp_x)\n",
    "#             self.projection_C(tmp_x, out = tmp_x)                       \n",
    "\n",
    "#             self.gradient.direct(tmp_x, out=p1)\n",
    "#             if isinstance (tau, (Number, np.float32, np.float64)):\n",
    "#                 p1 *= self.L/(self.regularisation_parameter * tau)\n",
    "#             else:\n",
    "#                 p1 *= self.L/self.regularisation_parameter\n",
    "#                 p1 /= tau\n",
    "\n",
    "#             if self.tolerance is not None:\n",
    "                \n",
    "#                 if k%5==0:\n",
    "#                     error = p1.norm()\n",
    "#                     p1 += self.tmp_q\n",
    "#                     error /= p1.norm()\n",
    "#                     if error<=self.tolerance:                           \n",
    "#                         should_break = True\n",
    "#                 else:\n",
    "#                     p1 += self.tmp_q\n",
    "#             else:\n",
    "#                 p1 += self.tmp_q\n",
    "#             if k == 0:\n",
    "#                 # preallocate for projection_P\n",
    "#                 self.pptmp = p1.get_item(0) * 0\n",
    "#                 self.pptmp1 = self.pptmp.copy()\n",
    "\n",
    "#             self.projection_P(p1, out=p1)\n",
    "                        \n",
    "\n",
    "#             t = (1 + numpy.sqrt(1 + 4 * t0 ** 2)) / 2\n",
    "            \n",
    "#             #tmp_q.fill(p1 + (t0 - 1) / t * (p1 - tmp_p))\n",
    "#             p1.subtract(tmp_p, out=self.tmp_q)\n",
    "#             self.tmp_q *= (t0-1)/t\n",
    "#             self.tmp_q += p1\n",
    "            \n",
    "#             tmp_p.fill(p1)\n",
    "\n",
    "#             if should_break:\n",
    "#                 break\n",
    "        \n",
    "#         #clear preallocated projection_P arrays\n",
    "#         self.pptmp = None\n",
    "#         self.pptmp1 = None\n",
    "        \n",
    "#         # Print stopping information (iterations and tolerance error) of FGP_TV     \n",
    "#         if self.info:\n",
    "#             if self.tolerance is not None:\n",
    "#                 print(\"Stop at {} iterations with tolerance {} .\".format(k, error))\n",
    "#             else:\n",
    "#                 print(\"Stop at {} iterations.\".format(k))                \n",
    "            \n",
    "#         if out is None:                        \n",
    "#             self.gradient.adjoint(self.tmp_q, out=tmp_x)\n",
    "#             tmp_x *= tau\n",
    "#             tmp_x *= self.regularisation_parameter \n",
    "#             x.subtract(tmp_x, out=tmp_x)\n",
    "#             return self.projection_C(tmp_x), self.tmp_q\n",
    "#         else:          \n",
    "#             self.gradient.adjoint(self.tmp_q, out = out)\n",
    "#             out*=tau\n",
    "#             out*=self.regularisation_parameter\n",
    "#             x.subtract(out, out=out)\n",
    "#             self.projection_C(out, out=out)\n",
    "#             return out, self.tmp_q\n",
    "    \n",
    "#     def convex_conjugate(self,x):     \n",
    "# #         tmp = np.maximum(self.proximal(x, tau)[1].pnorm(2).max()-alpha,0)\n",
    "# #         tmp = np.maximum( self.proximal(x, tau)[1]).pnorm(2).max()-1, 0)\n",
    "# #         tmp = self.gradient.adjoint(self.proximal(x, tau)[1]).max()\n",
    "#         print(self.proximal(x, tau)[1].pnorm(2).max()-1)\n",
    "# #         print(self.gradient.adjoint(self.proximal(x, tau)[1]).max()-1)\n",
    "# #         print(self.gradient.adjoint(self.proximal(x, tau)[1]).max()-1)\n",
    "#         return self.proximal(x, tau)[1].pnorm(2).max().sum()\n",
    "# #         print(self.proximal(x, tau)[1].sum())\n",
    "# #         return self.gradient.adjoint(self.proximal(x, tau)[1]).abs().sum()\n",
    "# #         return self.proximal(x, tau)[1].pnorm(2).sum()\n",
    "#     @property\n",
    "#     def L(self):\n",
    "#         if self._L is None:\n",
    "#             self.calculate_Lipschitz()\n",
    "#         return self._L\n",
    "#     @L.setter\n",
    "#     def L(self, value):\n",
    "#         warnings.warn(\"You should set the Lipschitz constant with calculate_Lipschitz().\")\n",
    "#         if isinstance(value, (Number,)) and value >= 0:\n",
    "#             self._L = value\n",
    "#         else:\n",
    "#             raise TypeError('The Lipschitz constant is a real positive number')\n",
    "\n",
    "#     def calculate_Lipschitz(self):\n",
    "#         # Compute the Lipschitz parameter from the operator if possible\n",
    "# #         Leave it initialised to None otherwise\n",
    "#         self._L = (1./self.gradient.norm())**2  \n",
    "    \n",
    "#     @property\n",
    "#     def gradient(self):\n",
    "#         '''creates a gradient operator if not instantiated yet\n",
    "#         There is no check that the variable _domain is changed after instantiation (should not be the case)'''\n",
    "#         if self._gradient is None:\n",
    "#             if self._domain is not None:\n",
    "#                 self._gradient = GradientOperator(self._domain, correlation = self.correlation, backend = self.backend)\n",
    "#         return self._gradient\n",
    "#     def __rmul__(self, scalar):\n",
    "#         if not isinstance (scalar, Number):\n",
    "#             raise TypeError(\"scalar: Expectec a number, got {}\".format(type(scalar)))\n",
    "#         self.regularisation_parameter = scalar\n",
    "#         return self\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cil21_ep]",
   "language": "python",
   "name": "conda-env-cil21_ep-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
